% Gemini theme
% https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
% 60in x  36in
% scale=1.2 corresponds to 
% - body text about 29pt
% - header text about 35pt
% - caption text about 25pt
\usepackage[size=custom,width=152.4,height=91.44,scale=1.2]{beamerposter} 
\usetheme{gemini}
\usecolortheme{rice}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}

\usepackage{algorithm}
\usepackage[indLines=false]{algpseudocodex}

% Macros
\newcommand{\gr}{\mathrm{gr}}
\newcommand{\defeq}{:=}
\newcommand{\eqdef}{=:}
\newcommand{\relu}{\sigma}
\newcommand{\littleo}{o}
\newcommand{\email}[1]{\texttt{#1}}
\renewcommand{\restriction}{|}
\def\Lambdamap{X}
\newcommand{\isdnet}{ISDnet}
\newcommand{\ODESolve}{\texttt{ODESolve}}
\newcommand{\SANN}{\texttt{SANN}}
\newcommand{\MAtt}{\mathrm{MAtt}}
\newcommand{\Att}{\mathrm{Att}}
\newcommand{\SoftMax}{\mathrm{SoftMax}}
\newcommand{\SArgMax}{\mathrm{SArgMax}}
\newcommand{\saxp}{\mathrm{saxp}}
\newcommand{\LayerNorm}{\mathrm{LayerNorm}}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\lrcolwidth}
\newlength{\mcolwidth}


\setlength{\sepwidth}{0.025\paperwidth}

% Equispaced
\setlength{\lrcolwidth}{0.3\paperwidth}
\setlength{\mcolwidth}{0.3\paperwidth}

% Larger middle column
% \setlength{\lrcolwidth}{0.25\paperwidth}
% \setlength{\mcolwidth}{0.4\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{Semialgebraic Neural Networks: From roots to representations}

\author{S. David Mis \inst{1} \and Matti Lassas \inst{2} \and Maarten V. de Hoop \inst{1}}

\institute[shortinst]{\inst{1} Rice University \samelineand \inst{2} University of Helsinki}

% ====================
% Footer (optional)
% ====================

\footercontent{
  \hfill
  \begin{minipage}[c]{3cm}
    \includegraphics[height=2cm]{logo_ICLR.pdf}
  \end{minipage}
  \begin{minipage}[c]{0.1\textwidth}
    ICLR 2025, Singapore
  \end{minipage}
  \hfill
  \href{mailto:david.mis@rice.edu}{david.mis@rice.edu}}
% (can be left out to remove footer)

% ====================
% Logo (optional)
% ====================

% use this to include logos on the left and/or right side of the header:
% % Stacked vertically
% \logoleft{%
%   \begin{minipage}[c]{10cm}%
%     \phantom{\quad\quad}\includegraphics[height=3cm]{logo_rice.pdf} \\[1cm]
%     \includegraphics[height=4cm]{logo_helsinki_horiz.pdf}
%   \end{minipage}}
% Side-by-side
\logoleft{
  \begin{minipage}[c]{30cm}
  \raisebox{1cm}{\includegraphics[height=4cm]{logo_rice.pdf}}
  \hspace{1cm}
  \includegraphics[height=6cm]{logo_helsinki.pdf}
  \end{minipage}
}
\logoright{\includegraphics[height=7cm]{qr-code.pdf}}


% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\lrcolwidth}

  \begin{exampleblock}{Abstract}
    Many numerical algorithms in scientific computing---particularly in areas like numerical linear algebra, PDE simulation, and inverse problems---produce outputs that can be represented by semialgebraic functions; that is, the graph of the computed function can be described by finitely many polynomial equalities and inequalities. 
    In this work, we introduce Semialgebraic Neural Networks (SANNs), a neural network architecture capable of representing any bounded semialgebraic function, and computing such functions up to the accuracy of a numerical ODE solver chosen by the programmer.
    Conceptually, we encode the graph of the learned function as the kernel of a piecewise polynomial selected from a class of functions whose roots can be evaluated using a particular homotopy continuation method.
    We show by construction that the SANN architecture is able to execute this continuation method, thus evaluating the learned semialgebraic function.
    Furthermore, the architecture can exactly represent even discontinuous semialgebraic functions by executing a continuation method on each connected component of the target function.
    Lastly, we provide example applications of these networks and show they can be trained with traditional deep-learning techniques.
  \end{exampleblock}

  \begin{block}{Background}
    \begin{itemize}
      \item A function is \textbf{semialgebraic} if its graph is defined by polynomial equalities and inequalitites; equivalently, if it can be computed fy finitely many operations $+$, $-$, $\times$, $\div$, $\sqrt[n]{\cdot}$, $\texttt{if}$ with polynomial decision boundaries. Examples: Piecewise polynomial and rational functions, solving linear/polynomial systems, polynomial optimization. (See eg. \cite{bochnakRealAlgebraicGeometry1998})
      \item The \textbf{inf-sup definable (ISD)} functions are those that can be computed by combining polynomials with $\max$ and $\min$ operations (equivalently, ReLU activations). The \emph{Pierce--Birkhoff conjecture} states all piecewise polynomials are ISD functions \cite{mahePierceBirkhoffConjecture1984}.
      \item A \textbf{homotopy continuation method} is a \emph{globally convergent} technique for numericall finding roots of polynomials. Roots of a difficult function $F$ are found by constructing a homotopy from a simpler function $G$; roots of $G$ are transformed into roots of $F$ by solving an ODE \cite{chenHomotopyContinuationMethod2015}.
    \end{itemize}
  \end{block}

  \begin{block}{Proof of representation theorem}

    \begin{columns}
      \begin{column}{0.29\textwidth}
        \vspace{2cm}
        \begin{figure}[t]
          \centering
          \includegraphics[width=\textwidth]{images/overview.png}       
        \end{figure}
      \end{column}
      \begin{column}{0.7\textwidth}

          \textbf{Step 1}: Encode the graph of $F$ as the kernel of some function $G$.

          \begin{itemize}
            \item[\checkmark] Define a class of functions $G$. {\color{c3} $ISD^1$}
            \item[\checkmark] Prove for every continuous semialgebraic $F$, there exists $G \;{\color{c3} \in ISD^1}$ such that $\gr(F) = \ker(G)$.
            \item[\checkmark] {\color{c3} Show MRNNs compute precisely the ISD functions.} 
          \end{itemize}

          \textbf{Step 2}: Devise a globally convergent {\color{blue}root-finding} method roots of $G$.

          \begin{itemize}
            \item[\checkmark]Introduce homotopy continuation methods for root finding.
            \item[\checkmark] Design a homotopy continuation method that works for all bounded semialgebraic functions.
          \end{itemize}

          \textbf{Step 3}: Design Semialgebraic Neural Networks (SANNs) capable of {\color{blue}executing} the root-finding method.

          \begin{itemize}
            \item[\checkmark] Define the SANN architecture.
            \item[\tiny\checkmark] Show it can trace the kernel of $H_{x,a}$
            \item[\tiny\checkmark] Extend to discontinuous semialgebraic functions
          \end{itemize}

      \end{column}
    \end{columns}

  \end{block}

\end{column}

\separatorcolumn

\begin{column}{\mcolwidth}

  \begin{alertblock}{Main results: We show}
    \begin{enumerate}
      \item Every bounded (possibly discontinuous) semialgebraic function $F$ is the solution of some ODE initial value problem corresponding to a \textbf{homotopy continuation method} for root finding.
      % \item \textbf{Representation theorem:} Every bounded (possibly discontinuous) semialgebraic function $F$ is representable as a \textbf{Semialgebraic Neural Networks (SANNs)} that numerically evaluates its associated ODE.
      \item \textbf{Semialgebraic Neural Networks (SANNs)} can learn $F$ by learning it's associated ODE. \textbf{Every bounded (possibly discontinuous) semialgebraic function $F$ can be represented by a SANN.}
    \end{enumerate}
\end{alertblock}

  \begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/heaviside_homotopy.png}
    \caption{
      Computing the Heaviside step function $F$ (plotted in red) using a homotopy method. 
      The graph of $F$ is a subset of the kernel of a higher-dimensional piecewise polynomial $G$.
      In the upper row, we plot the surface of a homotopy $H$ such that $H(x,y,0)=0$ is trivial to solve (upper left), and $H(x,y,1)=G$ (upper right).
      The kernel of $H$ is shown with a green line (red points are also on top of the green line).
      The bottom row shows the projection of $H$ onto to $xy$-plane.
      We compute $F(x)=y$ by following the kernel of $H$ from time $t=0$ to $t=1$, keeping $x$ fixed.
      Although the kernel of $G$ contains points outside the graph of $F$ (the visible parts of the green lines), these points are never encountered when computing $F$ by following the homotopy.
      This process captures the discontinuity exactly, including the isolated point at $(0,0)$. 
    }
  \end{figure}

  % \begin{columns}
  %   \begin{column}{0.1875\paperwidth}
  %   \end{column}
  %   \separatorcolumn
  %   \begin{column}{0.1875\paperwidth}
  %   \end{column}
  % \end{columns}

  \begin{block}{ODE system}
    Every bounded semialgebraic function is the solution to an ODE of the form:
    \begin{align}
      \dot z(s)&=\hbox{clamp-sol}(M(x,z(s),s),b(x,z(s),s)), \label{ODE-alt} \\
      z(0)&=0. \label{ODE-alt2}
    \end{align}
    Matrix $M(x,z,s)$ and vector $b(x,z,s)$ are the output of an ISD network ${\mathcal N}(x,z,s)$. The function $\textrm{clamp-sol}$ is defined 
    \begin{equation}
    \textrm{clamp-sol}(M,b) \defeq \begin{cases}
            \hbox{clamp}(M^{-1}b,-c_{\max},c_{\max}) & \text{if $M$ is invertible} \\
            0 & \text{otherwise.}
        \end{cases}
    \end{equation}
    The function ``$\textrm{clamp}$'' operates component-wise on each element of a vector, and is defined
    \begin{equation}
        \textrm{clamp}(a, low, high) \defeq \begin{cases} a, & a \in [low, high] \\
        low, & a < low \\
        high & a > high.
        \end{cases}
    \end{equation}
  \end{block}
\end{column}

\separatorcolumn

\begin{column}{\lrcolwidth}

  \begin{block}{Neural network architecture}
    \begin{columns}
      \begin{column}{0.7\textwidth}
        \begin{algorithm}[H]
          \caption{Evaluating a SANN}
          \label{alg-sann}
          \begin{algorithmic}[1]
              \Require $\mathcal{N} \in \isdnet(m, n, k)$ \\
              $x \in \mathbb{R}^m$ \\
              $c_{\max} \in \mathbb{R}_{\ge 0}$
              
              \Function {\SANN}{$\mathcal{N}$, $x$, $c_{\max}$} 
                  \Function{$\dot{z}$}{$z$, $s$}
                      \State $(M, b) \leftarrow \mathcal{N}(x, z, s)$ \label{line-Mb}
                      \If {$M$ is singular} \label{line-M-singular-1}
                          \State \Return $0$ \label{line-M-singular-2}
                      \Else 
                          \State \Return \texttt{clamp}$(M^{-1} b$, $-c_{\max}, c_{\max})$ \label{line-clamp}
                      \EndIf
                  \EndFunction
                  \State $(y, t) \leftarrow \ODESolve( \dot{z}, (0, 0) )$ \label{line-ODESolve}
                  \State \Return $y$
              \EndFunction
            \end{algorithmic}
          \end{algorithm}

          Architecture diagram for a SANN. The SANN outputs $y$ as a semialgebraic function of input $x$. Vectors $z_j$ are the current values of an ODE at timestep $j$. The time-derivative $\dot{z}_j$ is computed using a neural network $\mathcal{N}$ capable of computing ISD piecewise polynomials. $\mathcal{N}$ accepts the current ODE state $(z_j, j/N)$, as well as recurrent input $x$. The output of $\mathcal{N}$ is a matrix $M$ and vector $b$, from which $\dot{z}_j$ is computed using $\hbox{clamp-sol}(M, b)$ ($\dot{z}_i = M^{-1} b$ in the common case). $\hbox{ODE-step}$ is a single update of a numerical ODE solver. Finally, $\Pi$ is a projection operation such that $y$ is the first $n$ components of $z$. 
      \end{column}
      \begin{column}{0.3\textwidth}

        % \includegraphics[width=0.8\linewidth]{images/architecture_vert.001.png}

        \begin{figure}[t]
          \centering
          \includegraphics[width=0.8\linewidth]{images/architecture_vert.001.png}         
        \end{figure}
      \end{column}
    \end{columns}

  
    % \begin{figure}[t]
    % \centering
    %   \includegraphics[width=0.8\linewidth]{images/architecture.png}
    %   \caption{

    %   }
    % \end{figure}

    
  \end{block}

  \begin{block}{Numerical example: Matrix inversion}
    Figure: handcrafted example. Table: trained on data.
    \begin{figure}[h]
      \centering
      \begin{minipage}[c]{0.49\textwidth}
          \includegraphics[width=\linewidth]{images/jacobi_sann50.pdf}
      \end{minipage} \hfill
      \begin{minipage}[c]{0.5\textwidth}
          \caption{(left) Trajectories of a SANN that solves linear systems to machine precision. Given a matrix $X \in \mathbb{R}^{50 \times 50}$ and vector $b \in \mathbb{R}^{50}$, the network produces a vector $y \in \mathbb{R}^{50}$ such that $Xy = b$. Each line shows the trajectory of the output $y$ for a given input pair $(X, g)$ as the SANN's ODE is iteratively solved. At each timestep, the network performs a single Jacobi iteration update. The output of the network exactly matches the output of $100$ steps of Jacobi iteration.}
          \label{fig:jacobi_sann}
          \begin{table}[h]
            \centering
            \begin{tabular}{c|c|c}
              & \# parameters & $\| Xy - g \|_2$ \\ \hline
              MLP & $306,140$ & $0.166$ \\ \hline
              SANN & $\mathbf{291,720}$ & $\mathbf{0.101}$
            \end{tabular}
            \caption{Results of training a SANN and feed-forward neural network to solve linear systems. Given a $10 \times 10$ matrix $X$ and and vector $g$, the networks output $y$ such that $Xy \approx g$.}
          \end{table}
      \end{minipage}
    \end{figure}

  \end{block}

  \begin{block}{References}

    \nocite{*}
    \footnotesize{\bibliographystyle{plain}\bibliography{poster}}

  \end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}
